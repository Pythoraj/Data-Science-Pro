{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054ed9f8-6ab4-42e9-9fbd-fa35f075839b",
   "metadata": {},
   "source": [
    "**Question1.** Explain the basic components of a digital image and how it is represented in a computer. State the\n",
    "differences between grayscale and color images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a11bf1-88dd-4b5a-991d-884ee94b1db8",
   "metadata": {},
   "source": [
    "### Basic Components of a Digital Image\n",
    "\n",
    "A digital image is fundamentally a two-dimensional array of pixels, where each pixel represents the smallest unit of the image and contains information about its color or intensity. The primary components of a digital image include:\n",
    "\n",
    "1. **Pixels**:\n",
    "   - The term \"pixel\" is short for \"picture element.\" Each pixel is a discrete sample of the image and holds data that defines its color or brightness. In a grayscale image, each pixel represents a single intensity value, while in a color image, each pixel represents a combination of colors.\n",
    "\n",
    "2. **Color Models**:\n",
    "   - Digital images can be represented in different color models, which define how colors are represented and combined:\n",
    "     - **RGB (Red, Green, Blue)**: The most common color model for digital images. Each pixel is represented by three components corresponding to the intensity of red, green, and blue. When combined, these colors can represent a wide spectrum of colors.\n",
    "     - **CMYK (Cyan, Magenta, Yellow, Black)**: Primarily used in color printing, this model uses four color channels to represent colors.\n",
    "     - **HSV (Hue, Saturation, Value)**: A color model that represents colors in a way that is more intuitive to human perception, particularly useful in image processing tasks.\n",
    "\n",
    "3. **Resolution**:\n",
    "   - The resolution of a digital image is defined by the number of pixels in each dimension (width x height). Higher resolution images contain more pixels and can capture finer details, while lower resolution images may appear pixelated or blurred.\n",
    "\n",
    "4. **Bit Depth**:\n",
    "   - Bit depth refers to the number of bits used to represent the color of each pixel. It determines how many colors can be represented in the image:\n",
    "     - **1-bit**: Black and white (2 colors).\n",
    "     - **8-bit**: Grayscale (256 levels of gray).\n",
    "     - **24-bit (True Color)**: 16,777,216 colors (8 bits for each of the RGB channels).\n",
    "     - **32-bit**: Includes an alpha channel for transparency, allowing for even more colors.\n",
    "\n",
    "### Representation of a Digital Image in a Computer\n",
    "\n",
    "In a computer, a digital image is stored as a matrix (or array) of pixel values. For a grayscale image, this matrix is typically two-dimensional (height x width), where each entry corresponds to the intensity of a pixel. For a color image, the representation can be three-dimensional (height x width x color channels), where each pixel is represented by three values (R, G, and B).\n",
    "\n",
    "- **Example**:\n",
    "  - A 3x3 pixel grayscale image may be represented as:\n",
    "    ```\n",
    "    [[  0, 128, 255],\n",
    "     [ 64, 192, 128],\n",
    "     [255,  0,  64]]\n",
    "    ```\n",
    "  - A 3x3 pixel RGB image might be represented as:\n",
    "    ```\n",
    "    [[[255, 0, 0],   [0, 255, 0],   [0, 0, 255]],  # Row 1\n",
    "     [[255, 255, 0], [0, 255, 255], [255, 0, 255]],  # Row 2\n",
    "     [[128, 128, 128],[255, 255, 255],[0, 0, 0]]]    # Row 3\n",
    "    ```\n",
    "\n",
    "### Differences Between Grayscale and Color Images\n",
    "\n",
    "1. **Color Information**:\n",
    "   - **Grayscale Images**: These images contain only intensity information. Each pixel represents a shade of gray, with values typically ranging from 0 (black) to 255 (white) in an 8-bit image.\n",
    "   - **Color Images**: These images contain multiple channels (usually three in the RGB model). Each pixel is represented by three values (R, G, B), allowing for the representation of a broad spectrum of colors.\n",
    "\n",
    "2. **Data Representation**:\n",
    "   - **Grayscale Images**: Represented as a 2D array (height x width) of single intensity values.\n",
    "   - **Color Images**: Represented as a 3D array (height x width x 3) where each pixel has three values corresponding to the red, green, and blue channels.\n",
    "\n",
    "3. **File Size**:\n",
    "   - **Grayscale Images**: Typically smaller in size due to fewer bits per pixel (1 byte for 8-bit images).\n",
    "   - **Color Images**: Larger file sizes because they require more data per pixel (3 bytes for 24-bit images).\n",
    "\n",
    "4. **Usage**:\n",
    "   - **Grayscale Images**: Often used in applications where color is not critical, such as medical imaging, certain types of computer vision tasks, and document scanning.\n",
    "   - **Color Images**: Commonly used in photography, digital art, and applications where color representation is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea244eb4-1685-48c9-a49c-deeae07cfce0",
   "metadata": {},
   "source": [
    "**Question 2.** Define Convolutional Neural Networks (CNNs) and discuss their role in image processing.Describe the\n",
    "key advantages of using CNNs over traditional neural networks for image-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5dc82e-a746-4582-93b5-568bd1ddff1a",
   "metadata": {},
   "source": [
    "### Definition of Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a specialized class of deep neural networks designed for processing data that has a grid-like topology, such as images. They are particularly effective for image recognition, classification, and segmentation tasks due to their ability to capture spatial hierarchies and patterns in visual data.\n",
    "\n",
    "**Key Components of CNNs**:\n",
    "1. **Convolutional Layers**: The core building blocks of CNNs, where convolution operations are applied to the input data. These layers use filters (or kernels) that slide across the input image to create feature maps, capturing spatial features like edges, textures, and shapes.\n",
    "\n",
    "2. **Pooling Layers**: These layers reduce the dimensionality of the feature maps, retaining the most important information while discarding redundant features. Common pooling methods include Max Pooling and Average Pooling.\n",
    "\n",
    "3. **Fully Connected Layers**: After several convolutional and pooling layers, the feature maps are flattened and passed through one or more fully connected layers, where the final classification or prediction is made.\n",
    "\n",
    "4. **Activation Functions**: Non-linear functions (e.g., ReLU, Sigmoid) are applied after each convolutional and fully connected layer to introduce non-linearity, enabling the network to learn complex patterns.\n",
    "\n",
    "### Role of CNNs in Image Processing\n",
    "\n",
    "CNNs play a crucial role in various image processing tasks, including:\n",
    "\n",
    "- **Image Classification**: Assigning labels to images (e.g., identifying objects in photographs).\n",
    "- **Object Detection**: Identifying and locating multiple objects within an image.\n",
    "- **Image Segmentation**: Dividing an image into segments or regions to facilitate analysis (e.g., distinguishing between different objects).\n",
    "- **Face Recognition**: Identifying or verifying individuals based on facial features.\n",
    "- **Image Generation and Enhancement**: Used in tasks like super-resolution, style transfer, and image denoising.\n",
    "\n",
    "### Key Advantages of CNNs over Traditional Neural Networks\n",
    "\n",
    "1. **Parameter Sharing**:\n",
    "   - CNNs use the same filter (kernel) across the entire image, which significantly reduces the number of parameters compared to fully connected networks. This parameter sharing leads to faster training and less risk of overfitting.\n",
    "\n",
    "2. **Local Connectivity**:\n",
    "   - In CNNs, each neuron in the convolutional layer is only connected to a local region of the input image (receptive field). This local connectivity allows the network to focus on local patterns and features, making it highly effective for image processing.\n",
    "\n",
    "3. **Spatial Hierarchies**:\n",
    "   - CNNs can learn hierarchical representations of data. Lower layers learn simple features (like edges and textures), while deeper layers learn more complex features (like shapes and objects). This hierarchical learning mimics the way humans perceive images.\n",
    "\n",
    "4. **Robustness to Translation**:\n",
    "   - Because CNNs use local receptive fields and pooling layers, they are inherently more robust to translations and distortions in the input data. This means that the position of an object within an image has less impact on the network's ability to recognize it.\n",
    "\n",
    "5. **Efficient Use of Data**:\n",
    "   - CNNs require fewer training examples to achieve high performance due to their ability to generalize well from limited data. They can effectively learn from augmented datasets by applying transformations like rotation, translation, and flipping.\n",
    "\n",
    "6. **Automatic Feature Extraction**:\n",
    "   - Unlike traditional neural networks that often require handcrafted features, CNNs automatically learn relevant features during the training process. This capability simplifies the preprocessing steps and reduces the need for domain expertise in feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45771e3-8672-4a01-be1d-b9b06c77c17e",
   "metadata": {},
   "source": [
    "**Question 3.** Define convolutional layers and their purpose in a CNN.Discuss the concept of filters and how they are\n",
    "applied during the convolution operation.Explain the use of padding and strides in convolutional layers\n",
    "and their impact on the output size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2cfa0c-6844-45bd-b269-57813f05fec9",
   "metadata": {},
   "source": [
    "### Convolutional Layers in Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Definition**:\n",
    "Convolutional layers are the fundamental building blocks of Convolutional Neural Networks (CNNs). They apply a mathematical operation called convolution to the input data, typically an image, to extract features. A convolutional layer consists of a set of learnable filters (or kernels) that slide across the input data, performing convolution operations to produce feature maps.\n",
    "\n",
    "### Purpose of Convolutional Layers\n",
    "\n",
    "The main purposes of convolutional layers in CNNs include:\n",
    "\n",
    "1. **Feature Extraction**:\n",
    "   - Convolutional layers automatically learn and extract important features from the input data. As the filters slide over the input image, they capture spatial hierarchies of features such as edges, textures, and patterns. This hierarchical feature extraction is crucial for understanding the content of images.\n",
    "\n",
    "2. **Local Connectivity**:\n",
    "   - Each neuron in a convolutional layer is only connected to a local region of the input image (known as the receptive field). This local connectivity allows the network to focus on local patterns, which is particularly beneficial for image data where nearby pixels often have related information.\n",
    "\n",
    "3. **Parameter Sharing**:\n",
    "   - Convolutional layers use the same filter (kernel) across different spatial locations in the input image. This parameter sharing significantly reduces the number of parameters in the network, making it more computationally efficient and less prone to overfitting.\n",
    "\n",
    "4. **Dimensionality Reduction**:\n",
    "   - Although convolutional layers often increase the number of feature maps (due to multiple filters), they also reduce the spatial dimensions of the input data through strides and padding. This dimensionality reduction helps to decrease the computational load and enables the network to focus on more abstract features as it deepens.\n",
    "\n",
    "5. **Translation Invariance**:\n",
    "   - By design, convolutional layers provide some level of translation invariance, meaning that they can recognize patterns regardless of their position in the image. This property is essential for tasks like object detection and image classification, where the same object may appear in different locations.\n",
    "\n",
    "### How Convolutional Layers Work\n",
    "\n",
    "1. **Filters/Kernels**:\n",
    "   - Filters are small matrices (e.g., 3x3, 5x5) that contain weights learned during the training process. Each filter is designed to detect specific features. For example, one filter may detect horizontal edges, while another may detect vertical edges.\n",
    "\n",
    "2. **Convolution Operation**:\n",
    "   - The filter is applied to the input image by sliding it across the image spatially. At each position, the filter performs an element-wise multiplication with the portion of the image it covers, followed by summing the results to produce a single output value. This operation creates a feature map that highlights the presence of the feature represented by the filter.\n",
    "\n",
    "3. **Activation Function**:\n",
    "   - After the convolution operation, an activation function (typically ReLU) is applied to introduce non-linearity. This step allows the network to learn complex patterns beyond linear transformations.\n",
    "\n",
    "4. **Output Feature Map**:\n",
    "   - The result of the convolution operation, after applying the activation function, is a new feature map. This map retains the spatial relationships of the features detected by the filters, allowing subsequent layers to learn more complex representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a916bc-03e7-43cc-bc94-16d3b790be14",
   "metadata": {},
   "source": [
    "### Concept of Filters in Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Definition**:\n",
    "Filters (also known as kernels) are small matrices used in convolutional layers of Convolutional Neural Networks (CNNs) to detect specific features in input data, such as images. Each filter is designed to highlight certain patterns or characteristics of the input, such as edges, textures, or shapes.\n",
    "\n",
    "### Characteristics of Filters\n",
    "\n",
    "1. **Size**: \n",
    "   - Filters are typically small compared to the input image. Common sizes include 3x3, 5x5, or 7x7. The smaller size allows filters to focus on localized regions of the input data while retaining spatial relationships.\n",
    "\n",
    "2. **Depth**: \n",
    "   - For color images, filters have a depth equal to the number of channels in the input. For example, an RGB image has three channels (red, green, and blue), so a 3x3 filter will be a 3-dimensional matrix with dimensions 3x3x3.\n",
    "\n",
    "3. **Learnable Parameters**: \n",
    "   - The values in the filters are not fixed; they are learnable parameters that are adjusted during the training process. The CNN learns optimal filter weights through backpropagation, allowing it to detect features that are most relevant for the given task (e.g., classification or detection).\n",
    "\n",
    "### Application of Filters during the Convolution Operation\n",
    "\n",
    "The convolution operation involves several steps, where filters are applied to the input data to produce feature maps:\n",
    "\n",
    "1. **Sliding the Filter**:\n",
    "   - The filter is placed over the input image at the top-left corner and slides (or convolves) across the image both horizontally and vertically. The amount of movement is defined by a parameter called the **stride**. For example, a stride of 1 means the filter moves one pixel at a time.\n",
    "\n",
    "2. **Element-wise Multiplication**:\n",
    "   - At each position, the filter performs an element-wise multiplication with the corresponding portion of the input image it covers. This operation multiplies each value in the filter with the corresponding pixel value of the input.\n",
    "\n",
    "3. **Summing the Results**:\n",
    "   - After the element-wise multiplication, the results are summed up to produce a single output value. This value reflects how strongly the feature represented by the filter is present in that particular region of the input image.\n",
    "\n",
    "4. **Output Feature Map**:\n",
    "   - The output of the convolution operation at each position forms a new matrix known as the **feature map** or **activation map**. This feature map highlights the presence and intensity of the specific feature that the filter is designed to detect. \n",
    "\n",
    "5. **Applying Multiple Filters**:\n",
    "   - In practice, multiple filters are applied in parallel to the input image, each producing its own feature map. These feature maps are then stacked together to form the output of the convolutional layer. This stacking allows the network to capture various features at different spatial locations.\n",
    "\n",
    "6. **Activation Function**:\n",
    "   - After the convolution operation, an activation function (commonly ReLU) is typically applied to introduce non-linearity to the output feature maps. This step enables the network to learn complex patterns beyond simple linear relationships.\n",
    "\n",
    "### Example of the Convolution Operation\n",
    "\n",
    "Consider a simple example with a 5x5 grayscale image and a 3x3 filter:\n",
    "\n",
    "- **Input Image**:\n",
    "  ```\n",
    "  [[1, 2, 3, 0, 1],\n",
    "   [0, 1, 2, 3, 1],\n",
    "   [1, 0, 1, 2, 0],\n",
    "   [2, 1, 0, 1, 1],\n",
    "   [1, 2, 1, 0, 2]]\n",
    "  ```\n",
    "\n",
    "- **Filter**:\n",
    "  ```\n",
    "  [[1, 0, -1],\n",
    "   [1, 0, -1],\n",
    "   [1, 0, -1]]\n",
    "  ```\n",
    "\n",
    "- **Convolution Operation**:\n",
    "  - Place the filter over the top-left corner of the image and perform the convolution:\n",
    "    - Element-wise multiplication:\n",
    "      ```\n",
    "      [[1*1, 2*0, 3*-1],\n",
    "       [0*1, 1*0, 2*-1],\n",
    "       [1*1, 0*0, 1*-1]]\n",
    "      ```\n",
    "    - Sum the results to get a single value for that position in the feature map.\n",
    "  \n",
    "- **Resulting Feature Map**:\n",
    "  - Repeat this process by sliding the filter across the entire image, resulting in a smaller feature map that highlights vertical edges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5d136-754b-4f9c-a3bc-30cf6239a194",
   "metadata": {},
   "source": [
    "### Padding and Strides in Convolutional Layers\n",
    "\n",
    "**Padding** and **strides** are two important concepts in the context of convolutional layers in Convolutional Neural Networks (CNNs). They significantly influence how the convolution operation is performed and how the output feature maps are generated.\n",
    "\n",
    "### 1. Padding\n",
    "\n",
    "**Definition**: \n",
    "Padding refers to the practice of adding extra pixels around the borders of the input feature map (or image) before applying the convolution operation. This is done to control the spatial dimensions of the output feature map.\n",
    "\n",
    "**Types of Padding**:\n",
    "- **Valid Padding** (no padding):\n",
    "  - No additional pixels are added to the input. The output size is smaller than the input size, as the filter cannot be applied to the edges of the input.\n",
    "  \n",
    "- **Same Padding**:\n",
    "  - Enough pixels are added to ensure that the output feature map has the same spatial dimensions as the input feature map. This is achieved by adding padding symmetrically on all sides (top, bottom, left, right).\n",
    "\n",
    "**Impact on Output Size**:\n",
    "- With **valid padding**, the output size decreases because the filter cannot slide over the edges. The output dimensions can be calculated using the formula:\n",
    "  \\[\n",
    "  \\text{Output Size} = \\left( \\frac{\\text{Input Size} - \\text{Filter Size}}{\\text{Stride}} \\right) + 1\n",
    "  \\]\n",
    "- With **same padding**, the output size remains the same as the input size. This is beneficial when you want to preserve the spatial dimensions through multiple convolutional layers.\n",
    "\n",
    "### 2. Strides\n",
    "\n",
    "**Definition**:\n",
    "The stride is the number of pixels by which the filter moves across the input feature map during the convolution operation. It determines how far the filter moves at each step.\n",
    "\n",
    "**Impact on Output Size**:\n",
    "- When using a stride greater than 1, the filter skips some positions while sliding over the input, resulting in a smaller output feature map. The output dimensions can be calculated with the formula:\n",
    "  \\[\n",
    "  \\text{Output Size} = \\left( \\frac{\\text{Input Size} - \\text{Filter Size} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right) + 1\n",
    "  \\]\n",
    "\n",
    "### Examples\n",
    "\n",
    "Let’s consider a simple example with a 5x5 input feature map, a 3x3 filter, a stride of 1, and different padding scenarios.\n",
    "\n",
    "**Input Feature Map**:\n",
    "```\n",
    "[[1, 2, 3, 0, 1],\n",
    " [0, 1, 2, 3, 1],\n",
    " [1, 0, 1, 2, 0],\n",
    " [2, 1, 0, 1, 1],\n",
    " [1, 2, 1, 0, 2]]\n",
    "```\n",
    "\n",
    "#### Example 1: Valid Padding\n",
    "\n",
    "- **Filter**:\n",
    "  ```\n",
    "  [[1, 0, -1],\n",
    "   [1, 0, -1],\n",
    "   [1, 0, -1]]\n",
    "  ```\n",
    "\n",
    "- **Stride**: 1\n",
    "- **Padding**: 0 (valid padding)\n",
    "\n",
    "**Output Size Calculation**:\n",
    "- Input Size: 5 (width) \n",
    "- Filter Size: 3\n",
    "- Using the formula:\n",
    "  \\[\n",
    "  \\text{Output Size} = \\left( \\frac{5 - 3}{1} \\right) + 1 = 3\n",
    "  \\]\n",
    "- The output feature map will be 3x3.\n",
    "\n",
    "#### Example 2: Same Padding\n",
    "\n",
    "- **Stride**: 1\n",
    "- **Padding**: 1 (same padding, adding one pixel around the input)\n",
    "\n",
    "**Padded Input Feature Map**:\n",
    "```\n",
    "[[0, 0, 1, 2, 3, 0, 1, 0],\n",
    " [0, 1, 2, 3, 1, 0, 0, 0],\n",
    " [1, 0, 1, 2, 0, 0, 0, 0],\n",
    " [2, 1, 0, 1, 1, 0, 0, 0],\n",
    " [1, 2, 1, 0, 2, 0, 0, 0],\n",
    " [0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "```\n",
    "\n",
    "**Output Size Calculation**:\n",
    "- Input Size: 7 (due to padding, width becomes 7)\n",
    "- Using the formula:\n",
    "  \\[\n",
    "  \\text{Output Size} = \\left( \\frac{7 - 3}{1} \\right) + 1 = 5\n",
    "  \\]\n",
    "- The output feature map will remain 5x5.\n",
    "\n",
    "#### Example 3: Stride of 2\n",
    "\n",
    "- **Stride**: 2\n",
    "- **Padding**: 0 (valid padding)\n",
    "\n",
    "**Output Size Calculation**:\n",
    "- Using the formula:\n",
    "  \\[\n",
    "  \\text{Output Size} = \\left( \\frac{5 - 3}{2} \\right) + 1 = 2\n",
    "  \\]\n",
    "- The output feature map will be 2x2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69370aba-5cdf-4d6a-b6c8-b9c08a6f5294",
   "metadata": {},
   "source": [
    "**Question 4.** Describe the purpose of pooling layers in CNNs.Compare max pooling and average pooling operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379b460-4eda-4e51-80b9-90f54aa4c036",
   "metadata": {},
   "source": [
    "### Purpose of Pooling Layers in CNNs\n",
    "\n",
    "Pooling layers are an essential component of Convolutional Neural Networks (CNNs) that serve several important purposes:\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - Pooling layers reduce the spatial dimensions (width and height) of the input feature maps, which decreases the computational load for subsequent layers and helps to manage memory usage.\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - By summarizing the information in a feature map, pooling layers help retain the most salient features while discarding less important information. This makes the network more robust to variations in the input.\n",
    "\n",
    "3. **Translation Invariance**:\n",
    "   - Pooling provides some degree of translation invariance, meaning that small translations (shifts) in the input data will not significantly affect the output of the network. This property is beneficial for tasks like image classification, where the position of features can vary.\n",
    "\n",
    "4. **Control Overfitting**:\n",
    "   - By reducing the complexity of the model, pooling layers can help prevent overfitting, especially in networks with a large number of parameters.\n",
    "\n",
    "### Types of Pooling Operations\n",
    "\n",
    "The two most common pooling operations are **max pooling** and **average pooling**. Here’s a comparison of both:\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "**Definition**:\n",
    "Max pooling selects the maximum value from a specific region (window) of the input feature map.\n",
    "\n",
    "**Operation**:\n",
    "- A filter of size \\( k \\times k \\) (e.g., 2x2) slides over the input feature map with a defined stride.\n",
    "- At each position, it takes the maximum value from the covered region.\n",
    "\n",
    "**Example**:\n",
    "For a 2x2 max pooling operation on the following feature map:\n",
    "```\n",
    "[[1, 3, 2, 4],\n",
    " [5, 6, 7, 8],\n",
    " [9, 10, 11, 12],\n",
    " [13, 14, 15, 16]]\n",
    "```\n",
    "Applying a 2x2 max pooling with a stride of 2 results in:\n",
    "```\n",
    "[[6, 8],\n",
    " [14, 16]]\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Preserves important features and allows the network to focus on the most relevant activations.\n",
    "- Effective for retaining edge information in images.\n",
    "\n",
    "#### Average Pooling\n",
    "\n",
    "**Definition**:\n",
    "Average pooling computes the average value from a specific region (window) of the input feature map.\n",
    "\n",
    "**Operation**:\n",
    "- Similar to max pooling, but instead of selecting the maximum value, it calculates the average of all the values in the covered region.\n",
    "\n",
    "**Example**:\n",
    "For a 2x2 average pooling operation on the same feature map:\n",
    "```\n",
    "[[1, 3, 2, 4],\n",
    " [5, 6, 7, 8],\n",
    " [9, 10, 11, 12],\n",
    " [13, 14, 15, 16]]\n",
    "```\n",
    "Applying a 2x2 average pooling with a stride of 2 results in:\n",
    "```\n",
    "[[4.75, 6.75],\n",
    " [12.75, 14.75]]\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Provides a smoother representation of the feature map by averaging out activations.\n",
    "- Can be beneficial when a more generalized feature representation is desired.\n",
    "\n",
    "### Comparison of Max Pooling and Average Pooling\n",
    "\n",
    "| Feature                  | Max Pooling                                | Average Pooling                          |\n",
    "|--------------------------|--------------------------------------------|------------------------------------------|\n",
    "| **Operation**            | Selects the maximum value in the window    | Computes the average value in the window |\n",
    "| **Feature Retention**    | Retains strong activations (edges, textures)| Provides a smoother representation, less sensitive to noise |\n",
    "| **Sensitivity**          | More sensitive to the presence of features  | More sensitive to background noise       |\n",
    "| **Use Cases**           | Commonly used in image processing tasks    | Often used when preserving global information is important |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a40fb9-d226-47f6-a106-b35d684629b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
